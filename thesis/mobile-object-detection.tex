% !TeX spellcheck = en_GB
\documentclass[
			   fontsize=11pt,
               paper=a4,
               bibliography=totoc,
               idxtotoc,
               headsepline,
               footsepline,
               footinclude=false,
               BCOR=12mm,
               DIV=13,
               openany,   % using this removes blank pages around part / chapter starts.
%               oneside    % include this if you have to print only one page per sheet of paper.
               ]
               {scrbook}

\include{settings}
\usepackage{lipsum} % for filling pages with stuff

% -------------------------------------------------------------------------------
% -------------------------- Glosssary Customisation --------------------------
% -------------------------------------------------------------------------------
% For some reason this did not work when located in settings.tex
% Any links in resulting glossary will not be "clickable" unless you load the glossaries package after the hyperref package.

\usepackage{xparse}
\usepackage[acronym,toc]{glossaries}

\DeclareDocumentCommand{\newdualentry}{O{}D<>{}m m m m m } {
	\newglossaryentry{gls-#3}{
		name={#5},
		text={#5\glsadd{gls-#3}},
		description={#6},
		plural={#7},
		#1
	}
	\newacronym[see={[Glossary:]{gls-#3}},#2]{#3}{#4}{#5\glsadd{gls-#3}}
}

% use the \newdualentry command like this:
% \newdualentry{OWD}    																			% label
% 	{OWD}		            																				 % abbreviation
% 	{One-Way Delay} 	   																				% long form
% 	{The time a packet uses through a network from one host to another}	  % description
%   {OWDs}																							   % abbreviation in plural

\newcommand{\code}[1]{\lstinline[basicstyle = \ttfamily\small]{#1}} % wrap \lstinline because texstudio doesn't parse it properly

\makeglossaries
% -------------------------------------------------------------------------------
% ---------------------- Acronyms and Glossary Definition ---------------------
% -------------------------------------------------------------------------------

\newacronym{nnapi}{NNAPI}{Neural Networks API}

\newacronym{cv}{cv}{computer vision}

\newacronym{cnn}{cnn}{convolutional neural network}

\newacronym{resnet}{ResNet}{residual neural network}

\newacronym{ml}{ml}{machine learning}

\newacronym{ann}{ann}{artificial neural network}

\newacronym{adb}{adb}{android-developer-tools}
	
\newdualentry{api}
	{api}
	{application programming interface}
	{set of functions and procedures allowing the creation of applications that access the features or data of an operating system, application, or other service (Oxford Dictionary)}
	{apis}

\newdualentry{ecg}
	{ecg}
	{electrocardiogram}
	{time-voltage-graph of the electrical activity of the heart measured by electrodes touching the skin}
	{ecgs}

\newdualentry{eeg}
	{eeg}
	{electroencephalography}
	{measures electrical signals emitted by the brain to observe brain activity}
	{eegs}

\newglossaryentry{diff_privacy}
{
	name={differential privacy},
	description={property of a learning algorithm that uses personal data for training while guaranteeing that an individual's data cannot be reverse engineered by analysing the algorithm while potentially even having access to the data of all the other individuals. In other words, the differentially private algorithm behaves very similar (if not entirely identical) regardless of whether a particular personal information was used during training.}
}

% -------------------------------------------------------------------------------
% --------------------------------- Thesis Info ---------------------------------
% -------------------------------------------------------------------------------

% set title, authors and stuff for the cover
% docytype needs xspace because it is used within text.
\def\doctype{Bachelor's Thesis\xspace}

\def\studyProgram{Informatics}
\def\title{Implementing a Mobile App for Object Detection}

\def\titleGer{Entwicklung einer mobilen App zur Objekterkennung}
\def\author{David Drews}
% Prof
\def\supervisor{Univ.-Prof. Dr. Hans-Joachim Bungartz}
% PhD Candidate
\def\advisor{Severin Reiz, M.Sc.}
\def\date{15th of August 2021}

\begin{document}
\frontmatter
% -------------------------------------------------------------------------------
% ---------------------------------- COVERPAGE ------------------------------
% -------------------------------------------------------------------------------

% correct BCOR - undo at the end !!!
\def\bcorcor{0.15cm}
\addtolength{\hoffset}{\bcorcor}
\thispagestyle{empty}
\vspace{4cm}
\begin{center}
    \includegraphics[width=4cm]{templateStuff/tumlogo.pdf}\\[5mm]
    \huge DEPARTMENT OF INFORMATICS\\[5mm]
    \large TECHNICAL UNIVERSITY OF MUNICH\\[24mm]

    {\Large \doctype in \studyProgram}\\[20mm]
    {\huge\bf \title\par}
    \vspace{15mm}
    {\LARGE  \author}
    \vspace{10mm}
    \begin{figure}[h!]
        \centering
        \includegraphics[width=4cm]{templateStuff/informat.pdf}
   \end{figure}
\end{center}

\cleardoubleemptypage

% -------------------------------------------------------------------------------
% ---------------------------------- TITLEPAGE --------------------------------
% -------------------------------------------------------------------------------

\def\bcorcor{0.15cm}
\addtolength{\hoffset}{\bcorcor}
\thispagestyle{empty}
\vspace{10mm}
\begin{center}
    \includegraphics[width=4cm]{templateStuff/tumlogo.pdf}\\[5mm]
	\huge DEPARTMENT OF INFORMATICS\\[5mm]
	\large TECHNICAL UNIVERSITY OF MUNICH\\[24mm]
	{\Large \doctype in \studyProgram}\\[20mm]
	{\LARGE\textbf \title}\\[10mm]
	{\LARGE\textbf \titleGer}\\[10mm]
	\begin{tabular}{ll}
		\Large Author:      	& \Large \author \\[2mm]
		\Large Supervisor:  	& \Large \supervisor\\[2mm]
		\Large Advisor:			& \Large \advisor\\[2mm]
		\Large Submission Date:       		& \Large \date
	\end{tabular}
	\vspace{-1mm}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=4cm]{templateStuff/informat.pdf}
	\end{figure}
\end{center}

% undo BCOR correction
\addtolength{\hoffset}{\bcorcor}
\newpage

% -------------------------------------------------------------------------------
% ---------------------------------- DISCLAIMER -------------------------------
% -------------------------------------------------------------------------------

\cleardoubleemptypage

\thispagestyle{empty}
\vspace*{0.7\textheight}
\noindent
I confirm that this \MakeLowercase{\doctype} is my own work and I have documented all sources and material used.\\

\vspace{15mm}
\noindent
Munich, \date \hspace{5cm} \author
\cleardoubleemptypage

% -------------------------------------------------------------------------------
% ---------------------------------- ABSTRACT --------------------------------
% -------------------------------------------------------------------------------

\phantomsection
\addcontentsline{toc}{chapter}{Abstract}
\vspace*{2cm}
\begin{center}
    {\Large \textbf {Abstract}}
\end{center}
\vspace{1cm}

We migrate the entire code base of the Android application TUM-Lens from Java to Kotlin. This facilitates the future development of the app as it makes the code more concise and error-proof. Consequently, we elaborate on further advantages of the Kotlin language over Java and analyse how this migration lowered the lines of the existing code. Moreover, we expand the functionalities of the app by an object detection feature based on Google's open source deep learning framework TensorFlow Lite. The implementation follows in the previous TUM-Lens developer's footsteps and integrates the object detection to entirely work on-device so that no data needs to be exchanged with external servers. On the object detection theory side, we distinguish object detection from other visual machine learning tasks and survey a selection of modern deep learning architectures - both for backbone and detector networks. In addition, we study the mechanics of a specific model in detail, the SSD MobileNet v1, as this is the model applied to the object detection task in TUM-Lens. This thesis expands Maximilian Jokel's previous work \textit{Implementing a TensorFlow-Slim based Android app for image classification} (2020). \\

\noindent The repository containing the source code belonging to this thesis can be found here: \url{https://gitlab.lrz.de/dvrws/lens}

\cleardoublepage

% -------------------------------------------------------------------------------
% ------------------------------ TABLE OF CONTENTS -------------------------
% -------------------------------------------------------------------------------

\tableofcontents
\thispagestyle{empty}
\cleardoubleemptypage

% -------------------------------------------------------------------------------
% --------------------------------- MAIN MATTER ------------------------------
% -------------------------------------------------------------------------------

\mainmatter

\part{Introduction and Background Theory}

\chapter{Motivation} \label{chap:motivation}

The aim of this work was to further develop the Android app TUM-Lens \cite{lensApp}. Its core function is the analysis of images that are captured by the camera of the Android device and transmitted to the app as a live feed. With the completion of this work, the pre-existing image classification capabilities of the app are now complemented with object detection.

For an optimal user experience, the analysis of the images must take place in near real time. This is the only way to ensure that the analysis results displayed always match the current content of the camera feed, which can change very quickly due to panning of the camera by its user. While in many applications the analysis of image data can take place decentrally in powerful data centres, in the case of TUM-Lens the image analysis runs on the mobile device itself.

%\section{Increasing Computation Power on Mobile Devices} %TODO: will probably not elaborate here as 3 topics for motivation might be enough

\section{Growing Support for On-Device Machine Learning}

Support for the development of \gls{ml} and also in particular deep learning applications for mobile platforms is growing steadily and from different directions at the same time. Developer-friendly frameworks such as TensorFlow\footnote{\url{https://www.tensorflow.org/}}, developed by Google Brain, or PyTorch\footnote{\url{https://pytorch.org/}}, developed by Facebook's AI Research Lab, are among the best-known deep learning frameworks~\cite{dl_ranking_2018}. The releases of TensorFlow Lite\footnote{\url{https://www.tensorflow.org/lite}} 2017~\cite{tflite_release_verge_2017} and PyTorch Mobile\footnote{\url{https://pytorch.org/mobile/home/}} 2019~\cite{pytorch_release_2019} show that mobile platforms increasingly come into focus of companies providing \acrlong{ml} software. In recent years, device manufacturers and operating system developers also started to provide dedicated hardware and software components for mobile machine learning. Examples include Apple's Neural Engine~\cite{neural_engine_verge_2017}, unveiled in 2017, or Android's \gls{nnapi}~\cite{nnapi_devguide_2021}. Apple's Neural Engine is a hardware component optimised for \acrlong{ml} requirements. Android's \gls{nnapi}, on the other hand, is an Android C \gls{api} for efficient computation of \gls{ml} operations and provides a basic set of functions for higher-level \gls{ml} frameworks. As a result of these developments, it is becoming easier for developers to build \gls{ml} applications that run efficiently on mobile devices. This support was a major catalyst for the initial and further development of TUM-Lens in the context of two bachelor theses.

\section{Offline Usability} \label{sec:offline_usability}

TUM-Lens is more independent compared to many other \acrlong{ml} based apps as it does not require an internet connection to use it. Often, apps and services require a connection to the internet because of the nature of the task they are meant to perform. The Amazon voice assistant Alexa can answer simple voice commands to control smart home devices or check the time without having access to the world wide web and thus already uses on-device \acrlong{ml}. But even if Alexa could analyse and understand all voice commands locally, the request would still have to be forwarded to the Amazon servers in most cases. Due to the large number of possible queries, not all answers can be kept on the device, but must be retrieved from a data centre that has more storage capacity. Such queries include daily topics such as the weather report, traffic or the result of a sporting event. However, an internet connection is not required to use the full range of functions of TUM-Lens. All the information needed for image classification and object detection is stored locally on the device in the form of various already trained \gls{ann}. With the integration of the corresponding mobile frameworks, the image analysis can therefore be carried out locally on the device, making the app independent of an internet connection.

\section{Improved Privacy} \label{sec:privacy}

The use of on-device \gls{ml} provides a further mechanism for protecting personal data in the context of machine learning in addition to existing methods such as \gls{diff_privacy}. Due to the growing support for mobile \gls{ml} applications mentioned above, but also due to the continually increasing power of mobile devices~\cite{mobile_cpu_power}, not only the use of pre-trained \glspl{ann} becomes possible, but also the training of new \glspl{ann} on the mobile device itself becomes more and more feasible and relevant~\cite{liu19}. If the training process takes place locally on the device itself, no data needs to be transferred to external instances such as a company's servers. This makes it possible to develop applications that adapt more and more individually to the user as they are used, while guaranteeing a maximum level of data protection. An example of the development of such an application is DeepType~\cite{deepType}. DeepType attempts to predict the next word used when the user enters the keyboard. While every user initially starts with the same pre-trained version of the \gls{ann} used by DeepType, the application continues to train this \gls{ann} with each input and thus adapts more and more to the characteristic input behaviour of the user - all without the text inputs ever leaving the device.


\chapter{Important Concepts}

Everyone is talking about machine learning. It is already impossible to imagine our everyday life without the use of the term. Due to the multitude of contexts in which machine learning is spoken of, some justifiably and some unjustifiably, it is important to create a common understanding of some machine learning-related concepts in order to ensure a common understanding of the theoretical contents of this work.

%TODO: Maybe outline the different concepts that will be explained as their connection is loser than the other parts of the thesis.

\section{Machine Learning}

A popular definition of \gls{ml} is attributed to Arthur Samuel describing it as the "field of study that gives computers the ability to learn without being explicitly programmed"\footnote{Althought cited in popular machine learning material like Andrew Ng's \gls{ml} course at Stanford~\cite{mlCourseStan} the quote appears neither in Samuel's 1959~\cite{mlQuote1959} nor his 1967 paper~\cite{mlQuote1967}.}. \acrlong{ml} algorithms circumvent this need for explicit programming by improving an internal model through data. This process is called training and the data used to train the model is often regarded to as the model's experience~\cite{mlMitchell}. As depicted in figure \autoref{fig:mlClassification}, \gls{ml} can be divided into the subfields supervised learning, unsupervised learning, semi-supervised learning and reinforcement learning.

\begin{figure}[H]
	\centering
	\begin{tikzpicture}[node distance=4cm]
		\node[box] (ml) {\large{\textbf{Classification of Machine Learning}}};
		\node[box, above left of = ml] (supervised) {
			\begin{minipage}{5cm}
				\centering\textbf{Supervised Learning}
				\begin{itemize}
					\item \texttt{Regression}
					\item \texttt{Classification}
				\end{itemize}
			\end{minipage}
		};
		\node[box, above right of = ml] (unsupervised) {
			\begin{minipage}{5cm}
				\centering\textbf{Unsupervised Learning}
				\begin{itemize}
					\item \texttt{Clustering}
					\item \texttt{Association}
				\end{itemize}
			\end{minipage}
		};
		\node[box,  below right of = ml] (semisupervised) {
			\begin{minipage}{5cm}
				\centering\textbf{Semi-supervised Learning}
				\begin{itemize}
					\item \texttt{Classification}
					\item \texttt{Clustering}
				\end{itemize}
			\end{minipage}
		};

		\node[box, below left of = ml] (reinforcement) {
			\begin{minipage}{5cm}
				\centering\textbf{Reinformcement Learning}
				\begin{itemize}
					\item \texttt{Classification}
					\item \texttt{Control}
				\end{itemize}
			\end{minipage}
		};
		\draw[arrow, shorten >= 5pt, shorten <= 5pt] (ml) -- (supervised);
		\draw[arrow, shorten >= 5pt, shorten <= 5pt] (ml) -- (unsupervised);
		\draw[arrow, shorten >= 5pt, shorten <= 5pt] (ml) -- (semisupervised);
		\draw[arrow, shorten >= 5pt, shorten <= 5pt] (ml) -- (reinforcement);
	\end{tikzpicture}
	\caption[Classification of Machine Learning]{The field of \acrlong{ml} divided into subfields by the characteristics of the underlying learning process. Also indicates the learning problems that are typically tried to be solved by applying the respective learning process.}
	\label{fig:mlClassification}
\end{figure}

\subsection{Supervised Learning}
In supervised learning, the learning machine is provided with input data as well as the output that is expected for the given input~\cite{introSupervised}. In the classical case of spam filtering, the input can be a collection of emails and the expected output is a label attached to each email that either classifies it as spam or as non-spam. The learning machine is then fed all e-mails as input data and learns to recognise which information in the input is important to produce the correct classification. As the system knows the correct answer for each training input, it can process an email, predict whether or not it is spam, and then use the known answer to change its weights in a way that will make it more likely to lead to a correct prediction and less likely to lead to a false prediction the next time it is presented with similar input.

\subsection{Unsupervised Learning}
Detecting hidden patterns and structuring data is where unsupervised learning comes into play. Learners of this type do not need to be provided with an expected output while being trained~\cite{introUnsupervised}. A scenario for the application of unsupervised learning is the problem of dividing a customer base into subgroups in order to treat every subgroup according to their specific needs. An employee might help the machine learning system by providing the number of subgroups she wants the system to generate. The learner then builds up its representation of the internal structure of the entire data set with every input it processes. After having processed enough customers, it will most likely have identified the key metrics that distinguish customers into the different groups. 

\subsection{Semi-Supervised Learning}
One use for semi-supervised learning is cluster analysis, which was already used as an example in the previous paragraph. In the case of semi-supervised learning, the system no longer has to work out the different groups (also known as \textit{clusters}) from the unlabelled data alone. Instead, it can use a small set of already labelled customers as a reference and build its internal representation of the entire dataset (labelled and unlabelled) around the clusters indicated by the pre-labelled data. This is especially useful because in many domains collecting or creating labelled data is difficult, expensive, or both~\cite{introSemiSup}.

\subsection{Reinforcement Learning}

Reinforcement learning is "learning what to do - how to map situations to actions - so as to maximize a numerical reward signal"~\cite{introRL}. Systems are trained via reinforcement learning to learn how to behave in dynamic environments. The tasks in these environments can stretch from playing a video game~\cite{rlStarCraft} to driving an autonomous car~\cite{rlCars}. These exemplary tasks show two characteristics that distinguish reinforcement learning from the other subfields of \gls{ml}: The reward signal is often delayed and attribution to single actions is difficult. Only once a game is won or the car has arrived safely at its destination the system knows if all the decisions it made along the way lead to a positive outcome. \textit{Trial-and-error} is therefore a term that summarises this learning paradigm quite precisely.


\section{Artificial Neural Networks} \label{ssection:ann}

An \acrlong{ann} - often just referred to as neural network - is a data processing concept that is inspired by biological neurons and their interconnectivity. As figures \autoref{fig:2layeredANN} and \autoref{fig:3layeredANN} show the artificial neurons (also called \textit{nodes}) in an \gls{ann} are grouped in \textit{layers}. There are three important types of layers: The \textit{input layer}\footnote{Note that the input layer is not counted towards the total number of layers in an \gls{ann}.}, the \textit{output layer} and an arbitrary number of \textit{hidden layers} in between the input and output layer. Similar to neurons in human brains, nodes of different layers can be connected. In \glspl{ann}, the nodes exchange signals in the form of numbers. Each node outputs a number that is computed by applying a non-linear function to its inputs. The output signal can then be a new input for other nodes or it can be part of the result returned by the output layer. The connections between nodes are also known as \textit{edges} and typically carry a weight. In the case of \glspl{ann}, the training process that is typical for all machine learning systems is the adjustment of these connection weights. The weights and other variables of the \gls{ann} are grouped under the term \textit{parameters}. In summary, an \gls{ann} transforms an input vector into an output vector through a series of non-linear functions, where both the calculation of the output and the training process are characterised by the specific structure of the \gls{ann} and its parameters.

%TODO: Maybe introduce a view typical ANN architectures like feed forward, convolutional, recurring, etc.

\section{Deep Learning}

Deep learning is a subarea of machine learning. Deep learning is characterised by the use of \glspl{ann} with many hidden layers. The more hidden layers a network has, the deeper it is. The deeper a network is and the more nodes the network has per layer, the more complex the computations that the \gls{ann} can successfully perform~\cite{dlBookGoodf}. As the number of layers and nodes grows, so does the number of parameters. Their large number is the reason deep learning requires extensive amounts of data to provide adequate results compared to other sub-disciplines of machine learning. Networks of this genus have been given the ability to perform extraordinarily complex computations at the expense of a resource-intensive training process.

\begin{multicols}{2} % defines an environment with two columns
	\begin{figure}[H] % [H] for EXACTLY HERE
		\centering
		\includegraphics[height=3cm]{figures/ann1.jpeg}
		\caption[2-Layered ANN]{2-layered \gls{ann}. It is called fully connected as every node from the previous layer is connected to every node in the next layer. \\
			\tiny{Source:~\cite{annGraphics}}}
		\label{fig:2layeredANN} % labels always have to be placed after the caption
	\end{figure}
	
	\columnbreak    % start next column
	
	\begin{figure}[H] % [H] for EXACTLY HERE
		\centering
		\includegraphics[height=3cm]{figures/ann2.jpeg}
		\caption[3-Layered ANN]{3-layered \gls{ann}. In \glspl{ann}, nodes in one layer are connected to nodes in other layers but not to other nodes in the same layer. \\
			\tiny{Source:~\cite{annGraphics}}}
		\label{fig:3layeredANN} % labels always have to be placed after the caption
	\end{figure}
\end{multicols}

\chapter{Object Detection}

The field of \gls{cv} encompasses numerous distinct problems and an even larger number of potential solutions. In the following, object detection as a typical task in the \gls{cv} context is distinguished from other \gls{cv} tasks that are closest to it in terms of learning objectives. We then give an overview of a selection of modern object detectors including their internal structures and differentiating factors. We close this chapter with a detailed description of the model applied to the object detection task in TUM-Lens.

\section{How Object Detection Differs from Related Tasks} \label{sec:cvtasks}

\begin{figure}[H] % [H] for HERE
	\centering
	\includegraphics[width=\textwidth]{figures/detection_related_tasks.png}
	\caption[Typical Computer Vision Tasks]{Object detection differs conceptually from other related \gls{cv} tasks regarding spatial information, the concept of objects and the number of detections in a scene.\\
		\tiny{Source:~\cite{cvTasks}}}
	\label{fig:cvTasks} % labels always have to be placed after the caption
\end{figure}

\subsection{Semantic Segmentation}
In semantic segmentation, each pixel of an image is assigned a class. However, there are no objects. This means that if there are several objects of the same class in the image, all the associated pixels receive the same class label. Therefore, the different objects cannot be differentiated based on the result of the semantic segmentation.

\subsection{Image Classification}
In image classification, the result of the detection is a single class. In rare application variants, a bounding box for one object of the detected class is also returned - as a rule, however, no spatial extent is associated with the task of classification.

\subsection{Object Detection}
Object detection deals with the identification of any number of objects within an image. For each object, a class label and its position are returned as the coordinates of a rectangle enclosing the object. It is important here that, as depicted in Figure \autoref{fig:cvTasks}, several objects of the same class can also be recognised. In contrast to the  task of semantic segmentation, the different objects of the same class can be distinguished.

\subsection{Instance Segmentation}
The instance segmentation essentially fulfils a better variant of the object detection. Again, several objects of different classes are detected and the positions of the classes are also part of the output. However, the positions are not marked by bounding boxes as in object detection, but each pixel belonging to an object receives a label of this object. The objects are therefore even more sharply separated from the image areas that do not contain an object and, in contrast to semantic segmentation, individual objects of the same class remain distinguishable.

\section{Object Detection Frameworks}

State of the art object detection frameworks run predominantly on deep learning architectures~\cite{dlForDetection}. The bachelor's thesis preceding this work, \textit{Implementing a TensorFlow-Slim based Android app for image classification}~\cite{maxJokel}, already explains how \glspl{cnn} work and why they are the most important building blocks for deep learning frameworks solving perceptual tasks. This explanation will therefore not be pursued here once more. Instead, we survey a range of modern object detection frameworks. Each framework is defined by the combination of a backbone and a detector. The selection was not put together at random but is based on popularity and performance in standard object detection benchmarks~\cite{backbones}. %TODO: add reference for detection frameworks

\subsection{Backbones} \label{ssec:backbones}

The term \textit{backbone} in the context of \glspl{ann} might be used differently depending on the task pursued. In the case of visual tasks and this thesis, the backbone is the part of an object detection framework that extracts features from the input data. This encapsulation of the feature extraction task in its own set of \glspl{cnn} allows the designer of the object detection pipeline to swap and test different backbones for the task at hand~\cite{backbones}.

\begin{table}[H]
	\begin{tabularx}{\columnwidth}{C | C | C}
		\hline
		backbone					& first publication & detectors \\
		\hline \hline
		AlexNet						  & 2012 \cite{backboneAlexNet}			& HyperNet \cite{detectorHyperNet} \\
		\hline
		VGG-16						 & 2014 \cite{backboneVGG}			   & PFPNet-R512 [K] \\
		\hline
		GoogLeNet			   	   & 2014 \cite{backboneGoogLeNet}			 & YOLOv1 \cite{detectorYOLOv1} \\
		\hline
		ResNets					  	  & 2015 \cite{backboneResNet}		   & BlitzNet512 \cite{detectorBlitzNet}, CoupleNet \cite{detectorCoupleNet}, RetinaNet \cite{detectorRetinaNet},  Faster R-CNN [A], R-FCN ? \\
		\hline
		Inception-ResNet-V2   & tbd					  & Faster R-CNN G-RMI [23], Faster R-CNN with TDM [24] \\
		\hline
		DarkNet-19					& tbd					& YOLOv2 [J] \\
		\hline
		MobileNet					 & 2017 \cite{backboneMobileNet}					 & SSDv2 \\		
	\end{tabularx}
	\caption[Modern CNN Backbones and Detectors]{Overview of modern \gls{cnn} backbone networks and detectors that build upon them.}
	\label{tab:backbonesDetectors}
\end{table}

TO-DO: further sources: \\
 https://scholar.google.com/scholar?q=Dai%2C%20J.%2C%20Li%2C%20Y.%2C%20He%2C%20K.%2C%20Sun%2C%20J.%3A%20R-FCN%3A%20object%20detection%20via%20region-based%20fully%20convolutional%20networks.%20In%3A%20Lee%2C%20D.D.%2C%20Sugiyama%2C%20M.%2C%20Luxburg%2C%20U.V.%2C%20Guyon%2C%20I.%2C%20Garnett%2C%20R.%20%28eds.%29%20Advances%20in%20Neural%20Information%20Processing%20Systems%2029%2C%20pp.%20379%E2%80%93387.%20Curran%20Associates%2C%20Inc.%20%282016%29
J: http://arxiv.org/abs/1612.08242
%K: https://doi.org/10.1007/978-3-030-01228-1_15

\subsubsection{AlexNet}



\subsubsection{ResNet}

While the baseline \gls{ann} described in \autoref{ssection:ann} only connects adjacent layers, \glspl{resnet} are a special subclass of \gls{ann} that introduces \textit{shortcut connections}~\cite{backboneResNet}. Shortcut connection are edges in the network that skip one or more layers.

\subsubsection{MobileNet}
As the name suggests, MobileNets were developed especially for mobile or embedded computer vision applications. The key innovation of the MobileNet architecture is the combination of two elements:
\begin{enumerate}
	\item utilisation of depth-wise separable filters~\cite{depthSep}
	\item a network structure that enables the developer to scale the model size down by adjusting only two hyper-parameters
\end{enumerate}
As a MobileNet is a building block of the detection framework implemented in TUM-Lens, a more detailed description can be found in \autoref{ssec:mobilenetArchitecture}.


\subsection{Two-Stage Detectors}

This class of detectors separates the object detection task into two distinct stages~\cite{12stageSYNASC2018}. The network of the first stage (named Region Proposal Network (RPN) in the context of the R-CNN detector family) uses the image data to generate region proposals. The second stage is a separate network that takes these region proposals (or ROI - short for regions of interest), potentially decreases the final number of ROI using mechanics that depend on the specific detector, and then performs the classification on each of the final regions. The classified regions are then returned as the detected objects. Two-stage detectors tend to have a higher localization and object recognition accuracy than single-stage detectors but can only achieve that at the cost of considerably slower inference speed~\cite{surveyICBDT2019}. Table \autoref{tab:backbonesDetectors} shows a selection of popular detectors including R-CNN and some of its many successors, Fast-CNN and Faster-CNN.
%TODO make sure they really do appear in the tabel



\subsection{R-CNN}

R-CNN was 
Fast, Faster R-CNN, and other advancements like Mask-R-CNN
2014 \cite{detectorRCNN}

\subsection{Single-Stage Detectors}

Detection frameworks are considered to have only a single stage when they consist of one deep neural network only and compute the objects (bounding box coordinates and category) in a single pass through that network. By eliminating the explicit generation of region proposals, single-stage detectors outperform their two-stage competitors with respect to speed while sacrificing a bit of detection accuracy, if at all~\cite{detectorSSD}. %TODO: Cite more sources
This speed improvement makes single-stage detectors the preferred choice for applications running on mobile or embedded devices or in applications that require real-time image detection. Since the object detector implemented in TUM-Lens is described thoroughly in \autoref{sec:mobilessd}, the following overview of selected single-state detectors will help us to understand how the app's object detector differs from other possible options.

\subsubsection{RetinaNet}

2018

\subsubsection{YOLO}

YOLO (You Only Look Once) ; \cite{detectorYOLOv4}


%%%%
% Other modern and potentially interesting detectors
% CenterNet 2019
% EfficientDet 2019
% ShuffleNet 2018
% FBNet 2019
%%%%

\section{SSD MobileNet v1} \label{sec:mobilessd}

The SSD MobileNet v1 is the TensorFlow-Lite model used for the new object detection functionality integrated into TUM Lens. The model is pre-trained on the COCO dataset\footnote{\url{https://cocodataset.org/}} and available on TensorFlow Hub\footnote{\url{https://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/metadata/2}}. Following the data flow through the object detection framework, we will first have a closer look at the architecture of the MobileNet used as the backbone network. Secondly, we introduce the core concepts of the Single Shot MultiBox Detector (SSD) followed by an detailed description of the SSD architecture. Finally, we conclude with an outlook on the latest improvements to MobileNets, SSDs and their combined applications.

\subsection{MobileNet Architecture} \label{ssec:mobilenetArchitecture}
TBD More Intro \\
Firstly, using depth-wise separable filters leads to a significantly lower number of parameters the model needs to learn during the training phase. Secondly, the easy shrinking of the model by adjusting the two hyper-parameters (named \textit{width multiplier} and \textit{resolution multiplier}) allows the model-builder to scale the model down to exactly the size that is appropriate for solving the problem the model is applied to~\cite{backboneMobileNet}.

\subsection{Introduction to SSD}

The SSD consists of a backbone and the \textit{SSD head}. In theory, the backbone can be any of the networks mentioned in \autoref{ssec:backbones} although the choice is not limited to this selection. When SSD was first introduced  in 2015, the authors used the VGG-16 network as the backbone~\cite{detectorSSD}. In the case of the specific SSD variant implemented in TUM-Lens, MobileNet is used as a backbone. The reasons for the particular applicability of MobileNet in our Android app use case are described in \autoref{ssec:backbones}. Independent of the specific type of backbone, it is first trained on publicly available image data, e.g. from a database like ImageNet\footnote{\url{https://image-net.org/}}. Once the network is pre-trained the final layers that originally handled the classification are then replaced by the SSD head.

\subsection{The SSD Head}

The SSD head is the part of the detector that computes category scores and box locations. It is a characteristic of SSD to work with a fixed set of default bounding boxes. To account for the different sizes in which objects might appear in an image, SSD applies a sequence of comparatively small convulational filters to the feature map returned by the backbone network. Each of the feature maps obtained by this process represents a different receptive field and allows the network to detect objects at different scales. Consequently, SSD computes  predictions for all of these feature maps and for each of its predefined aspect ratios. It then fuses the predections of the different aspect ratios and scales in order to improve detection accuracy. The range of aspect ratios and scales, although predeterminded, enables the SSD to detect object in various shapes and sizes. Its multi-scale feature maps are a core differentiator from other single-stage detectors like YOLO~\cite{detectorSSD, detectorYOLOv1} .

\begin{figure}[H] % [H] for HERE
	\centering
	\includegraphics[width=\textwidth]{figures/ssd_feature_maps_scaling.png}
	\caption[Detecting Objects at Different Scales through SSD's Feature Maps]{The combination of bounding boxes in varying aspect ratios and multi-scale feature maps allows the SSD to detect objects in different sizes and orientations.\\
		\tiny{Source:~\cite{detectorSSD}}}
	\label{fig:ssdFeatureMapScaling} % labels always have to be placed after the caption
\end{figure}

\subsection{Architecture}

\subsection{Latest Improvements}


\part{App Development}

\chapter{Previous State of the Application} \label{sec:previousState}

The practical part of this work was built in top of the already existing Android application TUM-Lens in its version 1.0. In that version, the app is already cable of classifying images received from the live camera stream in real-time. It can also classify images that are loaded from the disk of the Android as an alternative operational mode to the camera stream classification. Moreover, the user can choose between different TensorFlow-Slim\footnote{\url{https://github.com/google-research/tf-slim}} models to classify the images. This enables the user to do two things: she can compare the speed of similar detectors and she can also change the type of objects that can be detected as some of the available networks were trained on different data and with different class labels~\cite{maxJokel}. The most impactful design decision of the former developer of TUM-Lens was to run the entire classification locally on the Android device.

\chapter{Design Updates}

We also took the chance to introduce some design updates to modernise the app. The new look of the \code{DetectionActivity} can be seen in \autoref{fig:designUpdate} and an overview of all newly introduced screens next to their counterpart from the previous app version (if applicable) can be found in \autoref{fig:appImages} in the appendix. The two major advances of the updated design are the slight transparency added to the bottom sheet and the expansion of the camera preview behind the status bar. The first update is not merely a UI decision. It also leads to a better user experience because the bottom sheet in the \code{DetectionActivity} can easily cover the entire screen. Still being able to see a bit of what the camera is currently capturing helps to understand e.g. the different classification results shown. The second update regarding the transparent status bar makes the experience of the app more immersive. In fact, we also implemented a truly full screen mode making the navigation bar transparent as well and extending the image preview to the entire area of the display. However this behaviour is reserved for a small number of special use cases like e.g. gaming or image gallery apps. Therefore, we reverted that change as soon as we realised that it did not fit to the use case of TUM-Lens.

\bigskip

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{figures/app_class_new_rot.png}
	\caption[Current Design of the App]{Current Design of the App}
	\label{fig:designUpdate}
\end{figure}

\chapter{Migration from Java to Kotlin}

During the Google I/O conference 2017 Google announced that they are making the programming language Kotlin a first-class citizen in Android~\cite{googleIO17}. Two years later, Google refined this statement annunciating that Android development will "be increasingly Kotlin-first" and, at the present day, the Google Developers website recommends developers to choose Kotlin when they start building a new Android app~\cite{kotlinFirst}. This alone can be reason enough to migrate an application to Kotlin - especially while its code base is still as manageable as the code base of TUM-Lens. To provide further explanations for the soundness of such a step we will first look at the advantages of Kotlin over Java - with particular emphasis on the context of TUM-Lens being developed by students as part of their final theses. After that, we look at the code conversion process from Java to Kotlin which is backed by powerful functionality integrated into Android Studio. We will also see an example for a pitfall that can be easily overlooked in such a semi-automated conversion process. In \autoref{sec:codeSizeAnalysis}, we will also analyse how the number of lines of code changed from the Java files in app version 1.0 compared to the current state of the project being fully migrated to Kotlin.

\section{Advantages of Kotlin}
Continuing the development of TUM-Lens in Kotlin has several advantages. As the is app currently exclusively developed by students, Kotlin's enhanced expressiveness and conciseness over Java makes it easier for students to understand the entire application within the scope of a bachelor's or master's thesis. Moreover, students can learn more in the given amount of time they spend working on the app because they can focus more strongly on the implementation of new features instead of having to write boilerplate Java code. As students are more error prone than professional developers they surpassingly benefit from the safety features integrated into the Kotlin language. The most common cause for crashes of Java applications is the null pointer exception~\cite{nullPointerSamebug, nullPointerOverops}. With Kotlin being a strongly typed language, a lot of these null pointer exceptions will be avoided because Kotlin's safety features help the developer to identify potential sources of such errors very easily. Google itself claims that Android apps are 20\% less likely to crash when they contain Kotlin code~\cite{kotlinFirst}.

\section{Converting Java Files to Kotlin Files}

Android Studio offers a convenient way to convert files from Java to Kotlin. On macOS, we first have to open the Java file we want to migrate in Android Studio. Next, we select \textit{Code} from the menu bar and then the menu item \textit{Convert Java File to Kotlin File}. If we have not yet configured Kotlin in your project, Android Studio will prompt us to either do so or abort the migration process. Once Kotlin is set up Android Studio will immediately start the conversion. At the end of the process we are prompted with a dialogue asking "Some code in the rest of your project may require corrections after performing this conversion. Do you want to find such code and correct it too?" which we confirm by clicking on the yes-button. They file has now been converted from Java to Kotlin. In some cases the resulting Kotlin code works right away but in the context of TUM-Lens manual adjustments were mandatory in every file because of several reasons. \\

For a start, Android Studio could not always infer all types automatically. With Kotlin being strongly typed it was necessary to restructure the code so that the compiler could know each variable's type or safely cast a variable to the appropriate type (called \textit{Smart Cast} in Kotlin). We solved this situation using two different approaches distinguished by the importance of the successful execution of the respective code block. In the case of non-critical functionality, using Kotlin's safe call operator \code{?.} to access properties that might be null was sufficient. When it came to core functions, wrapping such functionality in an additional safety check was the better option. This not only capacitates the compiler to be certain that a mutable property had not accidentally become null before being accessed. It also enables us to react appropriately if the safety check fails. \\

\begin{lstlisting}[style=standard, language=Kotlin, label=code:kotlinProperty, caption={Kotlin's declaration syntax for a mutable property. Getter and setter are optional. The property type is only optional when the compiler can infer it from the context (meaning either from the initializer or from the return type of the getter).}]
var <propertyName>[: <PropertyType>] [= <property_initializer>]
	[<getter>]
	[<setter>]
\end{lstlisting}

There is one part to Android Studio's built-in code migration tool-chain that actually tricked us into introducing a bug: the conversion of getters and setters. \autoref{code:kotlinProperty} shows the syntax for the declaration of a property in Kotlin. Every property has default public getters and setters if not explicitly defined otherwise. Java, on the other hand, conventionally defines methods like \code{public variableType getVariableName()} and \code{public void setVariableName(variableType newValue)} for every variable of a private class that shall be retrieved or overwritten from outside that class. \\

\autoref{code:bugSourceJava} contains the original Java code and \autoref{code:bugSolutionKotlin} shows the final solution that we implemented in addition to Kotlin's property accessors. The default Kotlin getter is not enough here as it would omit the crucial conversion task. However, this is exactly what happened during the auto-conversion: only the default getter was available after the conversion had successfully finished. As a result, other parts of the logic broke because the \code{rgbBytes} object could not be properly processed which, in turn, lead to the application not working as expected. In this special case, some accessors of the particular property were in need of the image conversion while other accessors were not. Therefore, we did not put the image conversion into a custom getter because we did not want to call \code{imageConverter!!.run()} every time the property was used. Retrospectively, placing it in the separate method \code{getConvertedRgbBytes()} has been a good decision for us.

\begin{lstlisting}[style=standard, language=Java, label=code:bugSourceJava, caption={Java code that lead to the introduction of a bug after using Android Studio's built-in Java to Kotlin conversion command. The bug was created when this code block was substituted with a default getter in Kotlin ommiting parts of its logic.}]
	protected int[] getRgbBytes() {
		imageConverter.run();
		return rgbBytes;
	}
\end{lstlisting}

\begin{lstlisting}[style=standard, language=Kotlin, label=code:bugSolutionKotlin, caption={Complementing the default Kotlin getter with this method solved the problem.}]
	protected fun getConvertedRgbBytes(): IntArray? {
		imageConverter!!.run()
		return rgbBytes
	}
\end{lstlisting}

\chapter{Implementing Object Detection Using the TensorFlow Lite Framework}

As described in \autoref{sec:previousState} the core feature of TUM-Lens v1.0 is image classification. Most notably, the classification task is not outsourced to some remote server but performed offline on the device itself. We adhere to this decision and built our logic on top of Google's TensorFlow Lite example app for object detection~\cite{tfSampleAppGuide, tfSampleAppRepo}. The detection task is therefore carried out locally using the same TensorFlow-Lite API as the image classification. We also integrate our detection results into the images received from the camera live-stream so that the user of the app can experience the object detection in real-time. This being said, there is some discrepancy to the existing set of functionalities built around the image classification. Only one model for object detection is currently available within the app and the detection logic cannot be applied to images that have been loaded from the storage of the device.

\section{Expansion of the Existing App Architecture}

In order to make the structure of the app more accessible to new developers we put the core classes responsible for image classification and object detection into two distinct packages. The project now includes the four packages \code{classification}, \code{detection}, \code{fragments} and \code{helpers}. Only the two base activities \code{StartScreenActivity} and \code{PermissionDeniedActivity} remained on the top-level. The UML diagram depicted in \autoref{fig:umlDetectionPackage} is a good starting point for interested people who want to familiarise themselves with the newly added contents of the app. Most of the code that was added is located within the \code{detection} package. However, adjustments in other parts of the app where necessary in order to integrate the new functionalities into the existing project.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{figures/uml_detection_package.png}
	\caption[UML Class Diagram of The New Detection Package]{UML class diagram of the new detection package. Only public properties and methods are shown.}
	\label{fig:umlDetectionPackage} % labels always have to be placed after the caption
\end{figure}


\section{Detection and Tracking}

Just as the \code{ClassificationActivity} is the core activity that handles the image classification of the camera feed, the new \code{DetectionActivity} is the central activity for object detection. The user can easily switch between the different mode of image analysis by using a toggle button on top of the screen. We decided to use a toggle switch here since there are exactly two operating modes (classification and detection) and they are working mutually exclusive in order to best utilise the computation power of the device. This design decision may be revised later when the scope of the app is expanded to cover more visual tasks (see \autoref{sec:cvtasks} for examples of such tasks). Another reason for a rework can be the fact that mobile devices might have enough computing power so that solving multiple tasks in parallel does not make a noticeable difference for the app user, meaning images are still processed in real-time. In this section, we will not explain the code step by step as this can be done be reviewing the code itself and reading the comments further explaining important parts it. Instead, we will explain a few key concepts and classes so that people scanning the code after reading this thesis know what to look for.

\subsection{Inheritance for Receiving and Rendering of Images}

The \code{DetectionActivity} inherits from the abstract class \code{CameraActivity} and implements the \code{OverlayView.DrawCallback} interface. The \code{CameraActivity} handles all communication with Androids camera \gls{api} and implements the proper setup and behaviour of the elements in the bottom sheet. The \code{DrawCallback} interface provides a single function definition that the \code{DetectionActivity} overrides in order to render the detected objects as rectangles in a layer on top of the preview of the camera image stream. The actual drawing is delegated to an \code{MultiBoxTracker} object as we will see in \autoref{ssec:detectionAndTracking}.

\subsection{Bitmap Conversion}

Since TUM-Lens can run on a multitude of devices with varying combinations of camera and display resolution, the \code{DetectionActivity} uses matrices to convert the image data back and forth between the different formats the data is needed in. For contemporary Android devices, the potential resolution of the camera is usually above the maximum resolution of the display. This is also the case for the Xiaomi Mi 9 on which we tested the application during its development (see \autoref{chap:specs} for its technical specifications). It is the developers' task to decide in which quality the camera input is displayed. Sticking to the example of Xiaomi Mi 9, the images captured by the camera have 20 (selfie camera) or 45 (main camera) megapixels and are scaled down to less than 2.5 megapixel to be displayed on the device screen. After that, they are again scaled down to a bitmap of 300x300 pixels\footnote{The image received by the detector might indeed be even smaller than 300x300 pixels as the camera input is usually not square and the aspect ratio is kept throughout the image conversion pipeline. Therefore, the 300 pixels denote the larger dimension (width or height) of the actual content of the input bitmap.} as this is the appropriate input size for the implemented detection framework described in \autoref{sec:mobilessd}. Once the detector returns bounding boxes for the objects detected in its input image the coordinates of these rectangles in turn are scaled back up to mark the corresponding areas in the higher resolution image shown on the device display.

\subsection{DetectionActivity and MultiBoxTracker} \label{ssec:detectionAndTracking}

The \code{DetectionActivity} also orchestrates the two most fundamental tasks of the object detection process: object detection and, since the images are derived from a continuos camera stream, object tracking. It instantiates the object \code{detector} of the class \code{TFLiteObjectDetectionAPIModel} to handle the interactions with the TensorFlow Lite \gls{api} and a \code{MultiBoxTracker} object \code{tracker} that tries to assign each rectangle drawn on the screen to its related recognition result across successive input images. This is important because the bounding boxes that are drawn as an overlay on the camera view use different colours for different objects. If no tracking would happen, the colour that is assigned to an object will change randomly with every new image the detector processes. This would be very confusing for the user. Therefore, the recognition results returned by the detector are not directly rendered on the screen but first processed by the \code{tracker}. \\

The \code{tracker} guarantees two properties: Firstly, empty or degenerated recognition results are skipped and not tried to be displayed. Secondly, the 15 colours the \code{tracker} can assign to the remaining objects are  allocated in the same order every time the old drawings are cleared from the canvas and new ones are drawn. This process of detecting objects, clearing old rectangles from the screen and drawing new ones happens with every image the detector processes. While the tracking is far from perfect, the order in which recognitions are returned by the detector stays roughly the same. This happens because they are ordered by the coordinates of their bounding boxes. Depending on the camera settings, the camera can usually stream 30 to 60 frames per second. If the user does not move the camera surpassingly fast, the coordinates of the recognition results stay close enough together so that they get assigned the same colour across the changing camera frames. New  objects appearing in the scene (or existing ones becoming larger as the user gets closer) are currently the major cause for inconsistent object colouring. However, as long as no new objects are recognised, the existing ones tend to keep their colours. This is already a great achievement given the tracking is purely based on properties of the TensorFlow Lite \gls{api} and does therefore add effectively non-existent overhead to the object detection pipeline. \\

The detection threshold is currently defined in a constant and set to 0.5 in order to display the most relevant objects. Hence, only recognition results received from the \code{detector} that have a confidence score of 0.5 or higher are handed to the \code{tracker}. This further improves the user experience supplementary to the object tracking.

\part{Results}

\chapter{Evaluation}

We are going to evaluate two metrics in this chapter. Firstly, we will have a look at the impact of changing the programming language from Java to Kotlin with regards to the number of lines of code. Secondly, the performance of the application is evaluated based on the goals that were set before we took on the topic of this thesis.

\section{Project Size after the Migration to Kotlin} \label{sec:codeSizeAnalysis}

Using the npm distribution of the tool Count Lines of Code (cloc)\footnote{Original cloc project by Al Danial: \url{https://github.com/AlDanial/cloc} \\ cloc npm distribution by Kent C. Dodds: \url{https://www.npmjs.com/package/cloc}} we can easily count lines of Java and Kotlin code while ignoring blank lines and comment lines. This enables us to monitor how the shift from Java to Kotlin influenced the number of lines of code. With Perl, Node and npm installed on our machine, we run the following command which prints its output to stdout:

\begin{lstlisting}[style=standard, language=bash, label=code:cloc, caption={npx cloc command with its options and arguments. Prints the lines of code analysis comapring files before and after the Java to Kotlin conversion. The output was also saved as cloc.csv and can be found in lens-david/thesis/raw\_data.}]
	(base) david@DDs-MBP lens-david % npx cloc --by-file --git-diff-rel --match-d='main' --include-lang=Java,Kotlin -csv f6a18e0f 031a26a7
\end{lstlisting}

\noindent The \code{--by-file} option enables us to directly compare each Java file with Kotlin equivalent as it changes the output so that the results are returned for every source file encountered. \code{--git-diff-rel} interprets the command line arguments as git targets and compares only files which have changed in either commit which is exactly what we need to measure the change in the number of lines of code. Only files in the main directory are of our concern so we let cloc only search this directory using the option \code{--match-d='main'}. This excludes e.g. the test directory from being searched. With \code{--include-lang=Java,Kotlin} we limit the output to files written in the languages we seek to collate. Other files like Android's XML layout files are not of interest for this analysis. Finally, we provide two git commits. \code{f6a18e0f} is the last commit pushed by the previous developer of TUM-Lens and \code{bf0dbf7f} is the more recent commit that does not contain Java files anymore but replaced them with their respective Kotlin substitute. \autoref{tab:cloc} shows the results of the cloc output. Overall, the total size of the original Java codebase shrank because of the migration to Kotlin. This is particularly impressive as understandably further logic was added to existing classes in order to account for the new object detection functionality.

\begin{table}[h]
	\begin{tabularx}{\columnwidth}
		{ >{\RaggedRight}p{10cm} | C }
		\hline
		Files in TUM-Lens v1.0 migrated from Java to Kotlin	&	Delta in lines of code	\\	\hline \hline
		CameraRoll	&	-34	\\	\hline
		Classifier	&	4	\\	\hline
		ListSingleton	&	-9	\\	\hline
		PermissionDenied	&	-8	\\	\hline
		StartScreen	&	-12	\\	\hline
		ViewFinder	&	-7	\\	\hline
		fragments/CameraRollPredictionsFragment	&	9	\\	\hline
		fragments/CameraSettingsFragment	&	-15	\\	\hline
		fragments/ModelSelectorFragment	&	5	\\	\hline
		fragments/PredictionsFragment	&	4	\\	\hline
		fragments/ProcessingUnitSelectorFragment	&	-7	\\	\hline
		fragments/SmoothedPredictionsFragment	&	6	\\	\hline
		fragments/ThreadNumberFragment	&	-6	\\	\hline
		helpers/App	&	1	\\	\hline
		helpers/CameraEvents	&	0	\\	\hline
		helpers/FreezeAnalyzer	&	-6	\\	\hline
		helpers/FreezeCallback	&	0	\\	\hline
		helpers/ImageUtils	&	43	\\	\hline
		helpers/Logger	&	6	\\	\hline
		helpers/ModelConfig	&	-2	\\	\hline
		helpers/ProcessingUnit	&	-2	\\	\hline
		helpers/Recognition	&	-26	\\	\hline
		helpers/ResultItem	&	-21	\\	\hline
		helpers/ResultItemComparator	&	-2	\\	\hline \hline
		\fontseries{b}\selectfont{cumulative delta over all relevant files}	&	\fontseries{b}\selectfont{-79}	\\ \hline
	\end{tabularx}
	\caption[Java Files in TUM-Lens v1.0 And Change in Lines of Code After Their Conversion to Kotlin]{This table shows the results from the command line prompt in \autoref{code:cloc}. Packages have been indicated as a prefix to the file name to resemble the original project structure of TUM-Lens v1.0 and file extension have been omitted.}
	\label{tab:cloc}
\end{table}

\section{Performance} \label{sec:performance}

One of the goals of this work is the expansion of TUM-Lens by integrating a real-time object detection feature running entirely on the device itself. With that in mind and judging by the updated version of the app we have produced, this goal is fully achieved. For those who want to test the new app and its performance themselves, the updated version of the app can be installed on an Android device by cloning our GitLab repository\footnote{\url{https://gitlab.lrz.de/dvrws/lens}}. We will also release the updated version 2.0 to the Google PlayStore\footnote{\url{https://play.google.com/store/apps/details?id=com.maxjokel.lens}} to make it accessible for a broader audience. Hereafter, we will outline how we conduct a tests in which we detect objects with TUM-Lens and log the detection timings to prove that the detection can indeed be regarded as performing in \textit{real-time}.

\subsection{Test Environment}
We measure the real-time characteristic of our implementation not only subjectively by assessing what we see on the screen when we run the app. We also track the time it takes the app between the moment the camera \gls{api} returns a new image and the bounding boxes of the recognitions returned by the detector are actually drawn on top of that image. While the detection time can vary based on the input data it needs to process, we did not try to measure this influence. However, we conduct our test in a private home which actually matches the types of categories the SSD MobileNet can detect quite well (e.g. bottle, cup, chair, couch, bed, dining table, tv, refrigerator, ...). This makes us confident of our analysis being viable for the assessment of the real-time quality of the object detection we implemented.

\subsection{Measuring Performance Using The Android System Trace}
We log the system trace on the test device directly\footnote{\url{https://developer.android.com/topic/performance/tracing/on-device}} (specs in \autoref{tab:specs}) because it allows us to walk around and capture different scenes without having to be attached to a computer. Once we are done collecting input, we attach the test phone to our computer and retrieve the data with the following command:

\begin{lstlisting}[style=standard, language=bash, label=code:adb, caption={The \gls{adb} offer a convenient way to retreive trace files saved on a Android device. This command transfers the system trace from our test device to the development machine it is attached to via an usb cable.}]
	(base) david@DDs-MBP lens-david % cd thesis/raw_data && adb pull /data/local/traces/ .
\end{lstlisting}

\noindent The command creates a new folder \textit{traces} in the given directory if it does not already exist. Since our device runs Android 10, the trace files saved in the new folder have the \code{.perfetto-trace} format. We use Google's open source trace analysis platform Perfetto\footnote{\url{https://perfetto.dev/}} to open and process the trace file we recorded while walking through the apartment. The Perfetto UI\footnote{\url{https://ui.perfetto.dev/}} offers a powerful and convenient tool to visualise the data stored in the trace file. Moreover, it also allows to query the data using SQL commands. Executed within the Perfetto UI, the query in \autoref{code:perfettoSql} returns a list of the three types of tracked events we are interested in: \code{imageAvailable}, \code{recogniseImage} and \code{DrawFrame} including their ids, names, a timestamp indicating the beginning of their execution (ts) and their entire duration (dur).

\begin{lstlisting}[style=standard, language=sql, label=code:perfettoSql, caption={SQL command to query object detection related events tracked with Android's system tracing.}]
	SELECT id, name, ts, dur
	FROM slice
	WHERE type = 'internal_slice' and
	(name = 'imageAvailable' or name = 'recognizeImage' or name = 'DrawFrame')
\end{lstlisting}

The source code includes carefully placed \code{Trace.beginSection([SectionName])} and \newline \code{Trace.endSection()} function calls so that these events are properly tracked. All timings were measured with the default configuration of using 4 CPU-cores for the object detection which can also be seen in \autoref{fig:perfetto} - the \textit{recognizeImage} slice is traced in the main detection thread and three further threads support it.

\bigskip

\begin{figure}[h] % [H] for HERE
	\centering
	\includegraphics[width=.97\textwidth]{figures/perfetto_visualisation.png}
	\caption[Screenshot of Visual Systrace Analysis With Perfetto UI]{The image shows a systrace visualisation using the Perfetto UI. The horizontal axis shows the relative time ()starting at 0 seconds) and the vertical axes lists important types of events. These events are sorted to reflect their chronology as close as possible.}
	\label{fig:perfetto}
\end{figure}

\newpage

We imported the output of the Perfetto SQL query into a Microsoft Excel spreadsheet\footnote{For people who do not have excel installed: Google Docs offers a spread sheet functionality under \url{https://docs.google.com/spreadsheets} that can read and manipulate .xlsx files as well} (located in the project repository under \code{thesis > raw\_data > traces > systrace.xlsx}. Two observations can be drawn from \autoref{fig:perfetto} that we want to explain because it puts the measurements into context and makes them easier to understand. \\

\begin{enumerate}
	\item The device renders images at an almost fixed rate taking whatever input is currently available. We used the systrace output to verify this and measured an average of 55 frames per seconds to be displayed on the screen. The \code{DrawFrame} tracking event can be found in in \autoref{fig:perfetto} in the row \textit{RenderThread 4025} (its name is cropped because of its short duration).
	\item The code that contains the \code{imageAvailable} section will be entered every time a new input image is returned from the camera \gls{api}. However, it only executes fully if the previous recognition process has already finished as only one image is processed at a time. In case a new image arrives while an old one is still being processed, the execution of the code that contains the \code{imageAvailable} trace is almost immediately terminated. This behaviour can also be visually tracked in \autoref{fig:perfetto}.
\end{enumerate}

\subsection{System Trace Analysis}

We look at the systrace entries of the \code{recognizeImage} event from two different angles. First, we calculate the mean of the duration of the \code{recognizeImage} task which is 36,110,754 nanoseconds (ns). This can be converted to potential frames a detector with this speed provides which is 28 frames per second (fps) on average. This is a decent performance and can definitely be considered real-time (as a reference: movies are usually recorded with 24 fps). Moreover, it is also close to how academic papers usually measure the performance of neural networks. However, this is not congruent with what the user experiences as it implies that the detector is working non-stop - which it does not. Thus, we measure a second metric to access the performance closer to what a user experiences. This second metric measures the average time from the start of one \code{recognizeImage} event to next. Thereby, we automatically include both the additional time needed to retrieve the image from the camera and also the downtime in which no detection is being computed but also no new image data is available. The average time for this metric is 61.500.325 ns translating to around 16 fps. This time is low enough for the bounding boxes to accurately track the corresponding objects and therefore qualifies as real-time. However, it is not low enough for the tracking to appear complete stutter-free which would be the ideal case. The average joint time of a non-aborted \code{imageAvailable} and its subsequent \code{recognizeImage} is 43.100.228 ns or 23 fps. In order to achieve the maximum visible performance on the test device, this time needs to be lowered to less than 15.640.005 ns (or more than 64 fps) which is the average time between \code{DrawFrame} events - while also being in perfect sync with the latter. This is a challenge to be taken on in a new project for someone who wants to optimise the current code for performance or when further detection frameworks become available.


\chapter{Outlook}

\section{Possible Applications}

As touched upon in \autoref{chap:motivation}, there are at least two major drivers that will fuel future development efforts. Protecting the sphere of privacy will definitively be a major motivation to build apps running machine learning frameworks locally. We have already introduced this idea by alleging the example of DeepType in \autoref{sec:privacy}. The second driver for an increase in on-device machine learning applications has been introduced in \autoref{sec:offline_usability}. It is the need for machine learners that can act independently of an internet connection. This is important as it is simply impossibly to guarantee such a connection in some circumstances. We will expand this line of thought by adding the requirement for increased speed that some challenges have. We will use the following paragraphs to introduce a wide range of potential applications centred around the idea of on-device machine learning partitioned by their major drivers.

\subsection{Autonomy and Speed Inspired Use Cases}

We will not go into too much detail in this section as this work emphasizes the importance of low latency performance for object detection on Android devices in multiple occasions already. Still, we want to highlight the case of on-device machine learning application requiring both autonomy and speed that will change our life in the most profound way.

\subsubsection{Autonomous Navigation}
Many applications in the field of autonomous navigation will greatly benefit from both aspects of on-device computation - autonomy and speed. Driving (or rather being chauffeured by) an autonomous vehicle is the showpiece of on-device machine learning. Firstly, it needs the fast reaction time provided by algorithms running on embedded hardware to minimise the distance and thereby time the data needs to travel when it is captured by the cars myriad of sensors. This facilitates the car's decision engine to make decisions in real-time. Given the high speed at which cars can travel and the unpredictability of other traffic participants, it is of utmost importance to react to changes in the environment in real-time. Secondly, a connection to some remote instance that analyses data online might not be given in all situations. Cars must pass tunnels, rural areas might have poor network coverage or the network itself might have an outage due to unforeseen events such as climate catastrophes, terrorist attacks or cyber attacks. In any of these cases, the car must not lose control but be able to continue to navigate smoothly and safely. The core of its data processing and machine learning pipeline must therefore function decoupled form all external networks. \\

Having said this, autonomous cars are not the only use case for navigation relying on on-device machine learning. We can also conceive other areas such as deep see exploration, drone navigation and even space travelling where either offline usage, minimal latency or both is required for successful autonomous navigation.

\subsection{Privacy Inspired Use Cases}

\subsubsection{Biometric Data}
Some company like Apple and Snap\footnote{Snap Inc. is the developer of Snapchat, a mobile app known for its various and peculiar filters that users can apply to manipulate the images captured by the smartphone camera.} are already heavily influenced and driven by privacy concerns - at least in specific areas of their business practices. Apple's Face ID is a well-known security feature of Apple's smartphone and table division. It relies purely on on-device machine learning to pick up all the important details that make a face unique and thereby successfully secures the device without transferring sensitive user data to the cloud. Snapchat uses on-device machine learning to power its manifold filters so that user data only gets transmitted once the user actively shares a photo or video via the app. Protecting biometric user data has already become a mainstream use case for big tech companies. However, these large firms can afford to employ a considerable amount of machine learning experts to ensure their artificial neural networks can also run on mobile devices with their limited storage and computing capabilities. Smaller firms, on the other hand, often do not have the capacity to hire many machine learning engineers. It is these firms that exceedingly benefit from on-device machine learning getting ever more accessible and increasingly easy to implement even for small teams.

\subsubsection{Health Data}
Since smartphones are just a subcategory of mobile devices, different hardware opens the door for new types of applications. Health care is an industry that can greatly benefit from on-device machine learning as health data is among the data categories that are most worthy of protection. Today, there is already an astonishing variety of devices that capture health data - each of which also comes in a "smart" version that not only collects this data but also sends it to decentralised servers for storage and processing: smart watches, scales, portable \gls{ecg} and \gls{eeg} devices, blood pressure monitors, glucometers, thermometers and many more. Future developments might combine more and more of such tools into personal health companion devices that can capture and track every aspect of our health condition. Combining these various inputs to a coherent assessment of one's health status is an ideal use case for the application of machine learning. Since the data is sensitive and can easily tell a story people might want to keep private, it is a reasonable assumption that there will be a market for health companions that perform their analysis offline without sharing any data with a third party.

\subsubsection{Further Dimensions}
Listing all application domains of on-device machine learning would exceed the scope of this work. It can be observed, however, that there is not only the dimension of machine learning task along which different applications can be found (image classification, object detection, natural language processing, ...) but also other dimensions such as hardware on which the machine learning algorithm is deployed (smartphones, gadgets, robots, ...) or industries benefiting from customers being less scared about privacy breaches (entertainment, media, public sector, ...).

%Organisation von Fotos auf dem Smartphone, ohne dass diese an die Server von Apple, \& Google Co geschickt werden müssen (Gruppieren von Fotos, die zu einem Urlaub gehören, Gesichtserkennung, Objekterkennung)

\section{Future Work}

To wrap up this thesis, we want to suggest a variety of topics that have not been covered by this work or its predecessor and might be of interest to students at TUM. By doing so, we hope to inspire other students to continue the development of TUM-Lens. In the eyes of the author, this app is a fertile ground for students striving for both challenging and interesting tasks ranging from state of the art mobile development up to training custom neural networks for mobile devices.

\subsubsection{Replace Deprecated Code and Fix Bugs}
A first improvement can be the replacement of deprecated code. We have already achieved great progress in modernising the app by migrating it to Kotlin but there is still a significant number of outdated code that can be modernised by the next student working on the TUM-Lens project. Unfortunately, the app is also not entirely bug free. One bug we have found in version 1.0 that we were not able to fix is the changeability of models once an image has been loaded from storage. This does currently not trigger a re-classification with the newly selected network and should be fixes in a future release.

\subsubsection{Enable Input Freezing and Analysis of Images Loaded from Storage}
These tasks primarily aim at aligning object detection features with those available for image classification. Going forward, a double tap while being in object detection mode shall thus freeze the detection process and keep the frozen results on screen. Furthermore, the user shall be able to select an image from the device storage and the app shall display the detected objects recognised for that specific input.

\subsubsection{Load New Models from Remote Servers and Increase Number of Available Models}
This might sound counter-intuitive to the goal of this thesis but downloading models and the corresponding label file from an online resource would further improve the usability of the app. Currently, a new app version needs to be published in order to add a new neural network to the app. This is the case because as of now all models must be saved to the assets folder of the application before it is built. Lastly, the user should have the option to choose from multiple object detection framework available within TUM-Lens just as in the image classification case.

\subsubsection{Expand The Set of Features of TUM-Lens}

The final tip is the expansion of the features of the app. With image classification and object detection we already cover to major disciplines in computer vision. In a follow-up project, instance segmentation could be implemented to further increase the capabilities of TUM-Lens. While this would add a entirely new feature, improvements to existing function are also possible. On the easier side of the enhancement spectrum are further customisation options for the user. In the current state of the app, the user can adjust the number of threads to utilise for carrying out the object detection tasks. More options like changing the minimum confidence threshold a detection must have to be displayed are also conceivable. Ranked among the more difficult advancements is the implementation of better tracking than the mechanism currently implemented and described in \autoref{ssec:detectionAndTracking}. \\

With every new contribution to TUM-Lens, the number of new directions in which one can take the app is only growing. We therefore encourage fellow students to follow suit and continue to evolve this project.



% -------------------------------------------------------------------------------
% ----------------------------------- APPENDIX --------------------------------
% -------------------------------------------------------------------------------

\appendix

\addtocontents{toc}{\protect\setcounter{tocdepth}{-2}}

\part{Appendix}

\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}

\chapter{Screenshots of the Application}

\begin{figure}[H]
	\centering
	\begin{subfigure}{.23\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figures/app_class_old.png}
		\caption[Screenshots of the previous app in version 1.0 showing the classification activity]{Classification v1.0}
		\label{fig:appImage11}
	\end{subfigure}
	\hfil
	\begin{subfigure}{.23\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figures/app_class_new.png}
		\caption[Screenshots of the new app in version 2.0 showing the classification activity]{Classification v2.0}
		\label{fig:appImage12}
	\end{subfigure}
	\hfil
	\begin{subfigure}{.23\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figures/app_detection_new.png}
		\caption[Screenshots of the new app in version 2.0 showing the detection activity]{Detection v2.0}
		\label{fig:appImage13}
	\end{subfigure}

	\bigskip
	\begin{subfigure}{.23\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figures/app_class_sheet_old.png}
		\caption[Screenshots of the previous app in version 1.0 showing the classification activity with its bottom sheet expanded]{Bottom sheet in classification v1.0}
		\label{fig:appImage21}
	\end{subfigure}
	\hfil
	\begin{subfigure}{.23\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figures/app_class_sheet_new.png}
		\caption[Screenshots of the new app in version 2.0 showing the classification activity with its bottom sheet expanded]{Bottom sheet in classification v2.0}
		\label{fig:appImage22}
	\end{subfigure} 
	\hfil
	\begin{subfigure}{.23\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figures/app_detection_sheet_new.png}
		\caption[Screenshots of the new app in version 2.0 showing the detection activity with its bottom sheet expanded]{Bottom sheet in detection v2.0}
		\label{fig:appImage23}
	\end{subfigure}
	\RawCaption{
		\caption[App visuals comparing version 1.0 and 2.0]{Development of app visuals from version 1.0 to 2.0. The bottom sheet is now slightly transparent and the camera preview stretches into the status bar.}
		\label{fig:appImages}
	}
\end{figure}


\chapter{Test Device Specifications} \label{chap:specs}

The development of TUM-Lens v2.0 was tested on a Xiaomi Mi 9. Development and testing occurred April to August 2021. The most relevant specifications are as follows. 

\begin{table}[h]
	\begin{tabularx}{\columnwidth}
		{ >{\RaggedRight}p{4cm} | L }
		\hline
		Brand and name	&	Xiaomi Mi 9	\\	\hline
		CPU	&	Octa-core: 8 Kryo 485 cores with respective clock speeds of 1x2.84 GHz, 3x2.42 GHz and 4x1.78 GHz	\\	\hline
		GPU	&	Adreno 640	\\	\hline
		RAM	&	6.00 GB	\\	\hline
		Screen resolution	&	1080 x 2340 pixels ($\approx$ 2.5 MP)	\\	\hline
		Screen aspect ratio	&	19.5:9 \\ \hline
		Screen size (diagonal)	&	6.39 inches	\\	\hline
		Main camera (triple)	&	48 MP, f/1.8, 27mm (wide), 1/2.0", 0.8µm, PDAF \newline 12 MP, f/2.2, 54mm (telephoto), 1/3.6", 1.0µm, PDAF \newline 16 MP, f/2.2, 13mm (ultrawide), 1/3.0", 1.0µm, PDAF	\\	\hline
		Selfie camera (single)	& 20 MP, f/2.0, (wide), 1/3", 0.9µm \\ \hline
		Model identifier	&	M1902F1G	\\	\hline
		MIUI version	&	MIUI Global 12.0.5 (QFAEUXM)	\\	\hline
		Android version	&	10 QKQ1.190825.002	\\	\hline
		Release Date	&	25th of March 2019	\\	\hline
	\end{tabularx}
	\caption[Test Device Specifications]{Test Device Specifications}
	\label{tab:specs}
\end{table}


\listoffigures

\listoftables

\printbibliography

\printglossary[type=acronym,nonumberlist]

\printglossary[type=main,nonumberlist]

\end{document}
